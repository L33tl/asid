<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>asid.automl_imbalanced.tools_ilc API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>asid.automl_imbalanced.tools_ilc</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import random
from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit
from sklearn.preprocessing import StandardScaler
from imblearn.pipeline import Pipeline
from imblearn.over_sampling import SMOTE, RandomOverSampler, ADASYN
from imblearn.under_sampling import RandomUnderSampler
import xgboost as xgb
from catboost import CatBoostClassifier
from sklearn.ensemble import RandomForestClassifier
import lightgbm as lgb
from hyperopt import fmin, tpe, space_eval
import pickle
import numpy as np
from sklearn.metrics import f1_score, accuracy_score, roc_auc_score, log_loss
from .abb import AutoBalanceBoost
from hyperopt import hp
from scipy.stats import rankdata
from datetime import datetime
from pathlib import Path

balance_dict = {&#34;SMOTE&#34;: SMOTE(random_state=42),
                &#34;RandomOverSampler&#34;: RandomOverSampler(random_state=42),
                &#34;ADASYN&#34;: ADASYN(random_state=42, n_jobs=-1),
                &#34;RandomUnderSampler&#34;: RandomUnderSampler(random_state=42)}

classificator_dict = {&#34;XGB&#34;: xgb.XGBClassifier(seed=10, verbosity=0, use_label_encoder=False),
                      &#34;RF&#34;: RandomForestClassifier(random_state=42, n_jobs=-1),
                      &#34;LGBM&#34;: lgb.LGBMClassifier(random_state=42),
                      &#34;catboost&#34;: CatBoostClassifier(random_seed=42, verbose=False)}

path = Path.cwd()
with open(&#34;/&#34;.join(str(path).split(&#34;\\&#34;))+&#39;/asid/automl_imbalanced/sampling_hyperparameters_space.pickle&#39;, &#39;rb&#39;) as f:
    space_dict = pickle.load(f)


def get_cv_type(split_num):
    &#34;&#34;&#34;
    Defines the type of splitting iterations.

    Parameters
    ----------
    split_num : int
        The number of splitting iterations.

    Returns
    -------
    cv_type : str
        The chosen type of splitting iterations.
    &#34;&#34;&#34;
    if split_num % 5 == 0:
        cv_type = &#34;kfold&#34;
    else:
        cv_type = &#34;split&#34;
    return cv_type


def scale_data(X_train):
    &#34;&#34;&#34;
    Fits scaler and applies it to the train sample.

    Parameters
    ----------
    X_train : array-like of shape (n_samples, n_features)
        Training sample.

    Returns
    -------
    X_train_scaled : array-like of shape (n_samples, n_features)
        Scaled sample.

    scaler : instance
        Fitted scaler.
    &#34;&#34;&#34;
    scaler = StandardScaler()
    scaler.fit(X_train)
    X_train_scaled = scaler.transform(X_train)
    return X_train_scaled, scaler


def get_sampl_strat_for_case(ss, count_class, balance_method):
    &#34;&#34;&#34;
    Calculates the sampling strategy parameter.

    Parameters
    ----------
    ss : float
        Sampling strategy parameter generated by Hyperopt.

    count_class : array-like
        The sorted unique values with the number of counts.

    balance_method : str
        Balancing procedure label.

    Returns
    -------
    ss_corr : float or dict
        The adjusted sampling strategy parameter.
    &#34;&#34;&#34;
    min_cl_arg = np.argmin(count_class[1])
    max_cl_arg = np.argmax(count_class[1])
    if len(count_class[0]) &gt; 2:
        if balance_method == &#34;RandomUnderSampler&#34;:
            new_dict = {}
            for i, val in enumerate(count_class[0]):
                if i == min_cl_arg:
                    new_dict[val] = count_class[1][i]
                else:
                    new_dict[val] = min(int(round(count_class[1][min_cl_arg] / ss, 0)), count_class[1][i])
        else:
            new_dict = {}
            for i, val in enumerate(count_class[0]):
                if i == max_cl_arg:
                    new_dict[val] = count_class[1][i]
                else:
                    new_dict[val] = max(int(round(count_class[1][max_cl_arg] * ss, 0)), count_class[1][i])
        ss_corr = new_dict
    else:
        ss_corr = max(count_class[1][min_cl_arg] / count_class[1][max_cl_arg], ss)
    return ss_corr


def calc_pipeline_acc(params, X, y, bal_alg, alg, metric):
    &#34;&#34;&#34;
    Evaluates the pipeline.

    Parameters
    ----------
    params : dict
        Parameters generated by Hyperopt.

    X : array-like of shape (n_samples, n_features)
        Training sample.

    y : array-like
        The target values.

    bal_alg : str
        Sampling procedure label.

    alg : str
        Ensemble classifier label.

    metric : str
        Metric that is used to evaluate the model performance.

    Returns
    -------
    score : float
        Evaluation of the model performance.
    &#34;&#34;&#34;
    score_list = []
    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    count = 0
    for train_index, test_index in skf.split(X, y):
        X_train, X_test = X[train_index], X[test_index]
        y_train, y_test = y[train_index], y[test_index]
        estimator = Pipeline([
            (&#39;balancing&#39;, balance_dict[bal_alg]),
            (&#39;classification&#39;, classificator_dict[alg])])
        if count == 0:
            count_class = np.unique(y_train, return_counts=True)
            params[&#34;balancing__sampling_strategy&#34;] = get_sampl_strat_for_case(params[&#34;balancing__sampling_strategy&#34;],
                                                                              count_class, bal_alg)
            if &#34;balancing__k_neighbors&#34; in params:
                params[&#34;balancing__k_neighbors&#34;] = int(params[&#34;balancing__k_neighbors&#34;])
            elif &#34;balancing__n_neighbors&#34; in params:
                params[&#34;balancing__n_neighbors&#34;] = int(params[&#34;balancing__n_neighbors&#34;])
        estimator.set_params(**params)
        X_train_scaled, scaler = scale_data(X_train)
        try:
            estimator.fit(X_train_scaled, y_train)
            X_test_scaled = scaler.transform(X_test)
            if metric in [&#34;roc_auc&#34;, &#34;log_loss&#34;]:
                pred = estimator.predict_proba(X_test_scaled)
                score_list.append(calc_metric(y_test, pred, metric))
            else:
                pred = estimator.predict(X_test_scaled)
                score_list.append(calc_metric(y_test, pred, metric))
        except:
            if metric == &#34;log_loss&#34;:
                score_list.append(np.inf)
            else:
                score_list.append(0)
        count += 1
    score = np.mean(score_list)
    if metric != &#34;log_loss&#34;:
        score = -score
    return score


def get_balance_params(X, y, bal_alg, alg, hyp_time, metric):
    &#34;&#34;&#34;
    Searches for optimal hyper-parameters for balancing procedure using Hyperopt.

    Parameters
    ----------
    X : array-like of shape (n_samples, n_features)
        Training sample.

    y : array-like
        The target values.

    bal_alg : str
        Sampling procedure label.

    alg : str
        Ensemble classifier label.

    hyp_time : int
        The runtime setting (in seconds) for Hyperopt optimization.

    metric : str
        Metric that is used to evaluate the model performance.

    Returns
    -------
    best : dict
        Optimal hyper-parameters for balancing procedure chosen by Hyperopt.
    &#34;&#34;&#34;
    count_class = np.unique(y, return_counts=True)
    space_ds = space_dict[bal_alg].copy()
    min_cl_arg = np.argmin(count_class[1])
    min_class = count_class[1][min_cl_arg] / np.max(count_class[1])
    space_ds[&#34;balancing__sampling_strategy&#34;] = hp.uniform(&#34;balancing__sampling_strategy&#34;, min_class, 1)
    best = fmin(fn=lambda params: calc_pipeline_acc(params, X, y, bal_alg, alg, metric), space=space_ds,
                algo=tpe.suggest, timeout=hyp_time, rstate=np.random.seed(42))
    best = space_eval(space_ds, best)
    if &#34;balancing__k_neighbors&#34; in best:
        best[&#34;balancing__k_neighbors&#34;] = int(best[&#34;balancing__k_neighbors&#34;])
    elif &#34;balancing__n_neighbors&#34; in best:
        best[&#34;balancing__n_neighbors&#34;] = int(best[&#34;balancing__n_neighbors&#34;])
    return best


def balance_exp(X, y, skf, bal_alg, alg, hyperopt_time, metric):
    &#34;&#34;&#34;
    Evaluates model performance on a range of splits.

    Parameters
    ----------
    X : array-like of shape (n_samples, n_features)
        Training sample.

    y : array-like
        The target values.

    skf : instance
        Splitting strategy instance.

    bal_alg : str
        Sampling procedure label.

    alg : str
        Ensemble classifier label.

    hyperopt_time : int
        The runtime setting (in seconds) for Hyperopt optimization.

    metric : str
        Metric that is used to evaluate the model performance.

    Returns
    -------
    score_list : list
        Model performance on a range of splits.

    time_list : list
         Model fitting and prediction time on a range of splits.
    &#34;&#34;&#34;
    score_list = []
    time_list = []
    for train_index, test_index in skf.split(X, y):
        time_dict = {}
        X_train, X_test = X[train_index], X[test_index]
        y_train, y_test = y[train_index], y[test_index]
        if hyperopt_time != 0:
            balance_params = get_balance_params(X_train, y_train, bal_alg, alg, hyperopt_time, metric)
            balance_params[&#34;balancing__sampling_strategy&#34;] = get_sampl_strat_for_case(
                balance_params[&#34;balancing__sampling_strategy&#34;], np.unique(y_train, return_counts=True), bal_alg)
        else:
            balance_params = None
        X_train_scaled, scaler = scale_data(X_train)
        estimator = Pipeline([
            (&#39;balancing&#39;, balance_dict[bal_alg]),
            (&#39;classification&#39;, classificator_dict[alg])])
        if balance_params:
            estimator.set_params(**balance_params)
        t0 = datetime.now()
        estimator.fit(X_train_scaled, y_train)
        time_dict[&#34;train_time&#34;] = datetime.now() - t0
        X_test_scaled = scaler.transform(X_test)
        if metric in [&#34;roc_auc&#34;, &#34;log_loss&#34;]:
            t0 = datetime.now()
            pred = estimator.predict_proba(X_test_scaled)
            time_dict[&#34;predict_time&#34;] = datetime.now() - t0
            score_list.append(calc_metric(y_test, pred, metric))
        else:
            t0 = datetime.now()
            pred = estimator.predict(X_test_scaled)
            time_dict[&#34;predict_time&#34;] = datetime.now() - t0
            score_list.append(calc_metric(y_test, pred, metric))
        time_list.append(time_dict)
    return score_list, time_list


def calc_metric(y_test, pred, metric):
    &#34;&#34;&#34;
    Calculates the evaluation metric.

    Parameters
    ----------
    y_test : array-like
        Correct target values.

    pred : array-like
        Predicted target values.

    metric : str
        Metric that is used to evaluate the model performance.

    Returns
    -------
    score : float
        Metric value.
    &#34;&#34;&#34;
    if metric.split(&#34;_&#34;)[0] == &#34;f1&#34;:
        if metric == &#34;f1_macro&#34;:
            score = f1_score(y_test, pred, average=&#34;macro&#34;)
        elif metric == &#34;f1_micro&#34;:
            score = f1_score(y_test, pred, average=&#34;micro&#34;)
        elif metric == &#34;f1_weighted&#34;:
            score = f1_score(y_test, pred, average=&#34;weighted&#34;)
    elif metric == &#34;accuracy&#34;:
        score = accuracy_score(y_test, pred)
    elif metric == &#34;roc_auc&#34;:
        if len(np.unique(y_test)) == 2:
            score = roc_auc_score(y_test, pred[:, 1])
        else:
            score = roc_auc_score(y_test, pred)
    elif metric == &#34;log_loss&#34;:
        score = log_loss(y_test, pred)
    return score


def abb_exp(X, y, skf, metric):
    &#34;&#34;&#34;
    Evaluates AutoBalanceBoost performance on a partial range of splits.

    Parameters
    ----------
    X : array-like of shape (n_samples, n_features)
        Training sample.

    y : array-like
        The target values.

    skf : instance
        Splitting strategy instance.

    metric : str
        Metric that is used to evaluate the model performance.

    Returns
    -------
    score_list : list
        Model performance on a range of splits.

    time_list : list
         Model fitting and prediction time on a range of splits.
    &#34;&#34;&#34;
    score_list = []
    time_list = []
    for train_index, test_index in skf.split(X, y):
        X_train, X_test = X[train_index], X[test_index]
        y_train, y_test = y[train_index], y[test_index]
        time_dict = {}
        model = AutoBalanceBoost()
        t0 = datetime.now()
        model.fit(X_train, y_train)
        time_dict[&#34;train_time&#34;] = datetime.now() - t0
        if metric in [&#34;roc_auc&#34;, &#34;log_loss&#34;]:
            t0 = datetime.now()
            pred = model.predict_proba(X_test)
            time_dict[&#34;predict_time&#34;] = datetime.now() - t0
            score_list.append(calc_metric(y_test, pred, metric))
        else:
            t0 = datetime.now()
            pred = model.predict(X_test)
            time_dict[&#34;predict_time&#34;] = datetime.now() - t0
            score_list.append(calc_metric(y_test, pred, metric))
        time_list.append(time_dict)
    return score_list, time_list


def fit_alg(cv_type, X, y, bal_alg, alg, hyperopt_time, split_num, metric):
    &#34;&#34;&#34;
    Evaluates model performance on a full range of splits.

    Parameters
    ----------
    cv_type : str
        The chosen type of splitting iterations.

    X : array-like of shape (n_samples, n_features)
        Training sample.

    y : array-like
        The target values.

    bal_alg : str or None
        Sampling procedure label.

    alg : str
        Ensemble classifier label.

    hyperopt_time : int
        The runtime setting (in seconds) for Hyperopt optimization.

    split_num : int
        The number of splitting iterations.

    metric : str
        Metric that is used to evaluate the model performance.

    Returns
    -------
    score_list : list
        Model performance on a range of splits.

    time_list : list
        Model fitting and prediction time on a range of splits.
    &#34;&#34;&#34;
    score_list = []
    time_list = []
    if cv_type == &#34;split&#34;:
        skf = StratifiedShuffleSplit(n_splits=split_num, test_size=0.2, random_state=42)
        if alg == &#34;AutoBalanceBoost&#34;:
            sub_score_list, sub_time_list = abb_exp(X, y, skf, metric)
        else:
            sub_score_list, sub_time_list = balance_exp(X, y, skf, bal_alg, alg, hyperopt_time, metric)
        score_list.extend(sub_score_list)
        time_list.extend(sub_time_list)
    else:
        random.seed(42)
        seed_val = random.sample(list(range(100000)), split_num // 5)
        for seed in seed_val:
            skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)
            if alg == &#34;AutoBalanceBoost&#34;:
                sub_score_list, sub_time_list = abb_exp(X, y, skf, metric)
            else:
                sub_score_list, sub_time_list = balance_exp(X, y, skf, bal_alg, alg, hyperopt_time, metric)
            score_list.extend(sub_score_list)
            time_list.extend(sub_time_list)
    return score_list, time_list


def fit_res_model(option_label, X, y, hyp_time, metric):
    &#34;&#34;&#34;
    Fits the resulting estimator.

    Parameters
    ----------
    option_label : str
        Classifier label.

    X : array-like of shape (n_samples, n_features)
        Training sample.

    y : array-like
        The target values.

    hyp_time : int
        The runtime setting (in seconds) for Hyperopt optimization.

    metric : str
        Metric that is used to evaluate the model performance.

    Returns
    -------
    model : instance
        Fitted estimator.

    scaler : instance
        Fitted scaler.
    &#34;&#34;&#34;
    if option_label == &#34;AutoBalanceBoost&#34;:
        model = AutoBalanceBoost()
        model.fit(X, y)
        scaler = None
    else:
        ol = option_label.split(&#34;+&#34;)
        bal_alg = ol[0]
        alg = ol[1]
        if hyp_time != 0:
            balance_params = get_balance_params(X, y, bal_alg, alg, hyp_time, metric)
        else:
            balance_params = None
        X_scaled, scaler = scale_data(X)
        model = Pipeline([
            (&#39;balancing&#39;, balance_dict[bal_alg]),
            (&#39;classification&#39;, classificator_dict[alg])])
        if balance_params:
            balance_params[&#34;balancing__sampling_strategy&#34;] = get_sampl_strat_for_case(
                balance_params[&#34;balancing__sampling_strategy&#34;], np.unique(y, return_counts=True), bal_alg)
            model.set_params(**balance_params)
        model.fit(X_scaled, y)
    return model, scaler


def choose_and_fit_ilc(self, X, y):
    &#34;&#34;&#34;
    Chooses the optimal classifier and fits the resulting estimator.

    Parameters
    ----------
    X : array-like of shape (n_samples, n_features)
        Training sample.

    y : array-like
        The target values.

    Returns
    -------
    classifer : instance
        Optimal fitted classifier.

    option_label : str
        Optimal classifier label.

    score : float
        Averaged out-of-fold value of eval_metric for the optimal classifier.

    scaler : instance
        Fitted scaler that is applied prior to classifier estimation.

    score_dict : dict
        Score series for the range of estimated classifiers.
    &#34;&#34;&#34;
    res_dict = {}
    score_dict = {}
    time_dict = {}
    cv_type = get_cv_type(self.split_num)
    option_list = []
    for alg in [&#34;catboost&#34;, &#34;RF&#34;, &#34;LGBM&#34;, &#34;XGB&#34;]:
        for bal_alg in [&#34;RandomOverSampler&#34;, &#34;SMOTE&#34;, &#34;RandomUnderSampler&#34;, &#34;ADASYN&#34;]:
            score_list, time_list = fit_alg(cv_type, X, y, bal_alg, alg, self.hyperopt_time, self.split_num, self.eval_metric)
            option_list.append(bal_alg + &#34;+&#34; + alg)
            score_dict[option_list[-1]] = score_list
            res_dict[option_list[-1]] = np.mean(score_list)
    score_list, time_list = fit_alg(cv_type, X, y, None, &#34;AutoBalanceBoost&#34;, self.hyperopt_time, self.split_num, self.eval_metric)
    option_list.append(&#34;AutoBalanceBoost&#34;)
    score_dict[option_list[-1]] = score_list
    time_dict[option_list[-1]] = time_list
    res_dict[option_list[-1]] = np.mean(score_list)
    if self.eval_metric == &#34;log_loss&#34;:
        res_dict = {k: v for k, v in sorted(res_dict.items(), key=lambda item: (item[1]))}
    else:
        res_dict = {k: v for k, v in sorted(res_dict.items(), key=lambda item: (item[1]), reverse=True)}
    option_label = list(res_dict.keys())[0]
    score = list(res_dict.values())[0]
    classifier, scaler = fit_res_model(option_label, X, y, self.hyperopt_time, self.eval_metric)
    return classifier, option_label, score, scaler, score_dict, time_dict


def calc_leaderboard(self):
    &#34;&#34;&#34;
    Calculates the leaderboard statistics.

    Returns
    -------
    ls : dict
        The leaderboard statistics that includes sorted lists in accordance with the following indicators:
        &#34;Mean score&#34;, &#34;Mean rank&#34;, &#34;Share of experiments with the first place, %&#34;,
        &#34;Average difference with the leader, %&#34;.
    &#34;&#34;&#34;
    ls = {}
    mean_dict = {}
    sub_rank_list = [[] for el in list(range(self.split_num))]
    for el in list(self.evaluated_models_scores_.keys()):
        mean_dict[el] = np.mean(self.evaluated_models_scores_[el])
        for i, val in enumerate(self.evaluated_models_scores_[el]):
            if self.eval_metric == &#34;log_loss&#34;:
                sub_rank_list[i].append(val)
            else:
                sub_rank_list[i].append(-val)
    if self.eval_metric == &#34;log_loss&#34;:
        mean_dict = {k: v for k, v in sorted(mean_dict.items(), key=lambda item: (item[1]))}
    else:
        mean_dict = {k: v for k, v in sorted(mean_dict.items(), key=lambda item: (item[1]), reverse=True)}
    leader = list(mean_dict.keys())[0]
    ls[&#34;Mean score&#34;] = mean_dict
    model_rank = []
    for i in range(len(sub_rank_list)):
        model_rank.append(rankdata(sub_rank_list[i], method=&#39;dense&#39;))
    model_rank = np.array(model_rank)
    rank_dict = {}
    leader_share = {}
    diff_dict = {}
    for i, el in enumerate(list(self.evaluated_models_scores_.keys())):
        rank_dict[el] = np.mean(model_rank[:, i])
        leader_share[el] = model_rank[model_rank[:, i] == 1, i].shape[0] / model_rank.shape[0] * 100
        if el != leader:
            diff = (np.array(self.evaluated_models_scores_[el]) - np.array(
                self.evaluated_models_scores_[leader])) / np.array(self.evaluated_models_scores_[leader]) * 100
            diff_dict[el] = np.mean(diff)
    rank_dict = {k: v for k, v in sorted(rank_dict.items(), key=lambda item: (item[1]))}
    leader_share = {k: v for k, v in sorted(leader_share.items(), key=lambda item: (item[1]), reverse=True)}
    diff_dict = {k: v for k, v in sorted(diff_dict.items(), key=lambda item: (item[1]), reverse=True)}
    ls[&#34;Mean rank&#34;] = rank_dict
    ls[&#34;Share of experiments with the first place, %&#34;] = leader_share
    ls[&#34;Average difference with the leader, %&#34;] = diff_dict
    return ls</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="asid.automl_imbalanced.tools_ilc.abb_exp"><code class="name flex">
<span>def <span class="ident">abb_exp</span></span>(<span>X, y, skf, metric)</span>
</code></dt>
<dd>
<div class="desc"><p>Evaluates AutoBalanceBoost performance on a partial range of splits.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>array-like</code> of <code>shape (n_samples, n_features)</code></dt>
<dd>Training sample.</dd>
<dt><strong><code>y</code></strong> :&ensp;<code>array-like</code></dt>
<dd>The target values.</dd>
<dt><strong><code>skf</code></strong> :&ensp;<code>instance</code></dt>
<dd>Splitting strategy instance.</dd>
<dt><strong><code>metric</code></strong> :&ensp;<code>str</code></dt>
<dd>Metric that is used to evaluate the model performance.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>score_list</code></strong> :&ensp;<code>list</code></dt>
<dd>Model performance on a range of splits.</dd>
<dt><strong><code>time_list</code></strong> :&ensp;<code>list</code></dt>
<dd>Model fitting and prediction time on a range of splits.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def abb_exp(X, y, skf, metric):
    &#34;&#34;&#34;
    Evaluates AutoBalanceBoost performance on a partial range of splits.

    Parameters
    ----------
    X : array-like of shape (n_samples, n_features)
        Training sample.

    y : array-like
        The target values.

    skf : instance
        Splitting strategy instance.

    metric : str
        Metric that is used to evaluate the model performance.

    Returns
    -------
    score_list : list
        Model performance on a range of splits.

    time_list : list
         Model fitting and prediction time on a range of splits.
    &#34;&#34;&#34;
    score_list = []
    time_list = []
    for train_index, test_index in skf.split(X, y):
        X_train, X_test = X[train_index], X[test_index]
        y_train, y_test = y[train_index], y[test_index]
        time_dict = {}
        model = AutoBalanceBoost()
        t0 = datetime.now()
        model.fit(X_train, y_train)
        time_dict[&#34;train_time&#34;] = datetime.now() - t0
        if metric in [&#34;roc_auc&#34;, &#34;log_loss&#34;]:
            t0 = datetime.now()
            pred = model.predict_proba(X_test)
            time_dict[&#34;predict_time&#34;] = datetime.now() - t0
            score_list.append(calc_metric(y_test, pred, metric))
        else:
            t0 = datetime.now()
            pred = model.predict(X_test)
            time_dict[&#34;predict_time&#34;] = datetime.now() - t0
            score_list.append(calc_metric(y_test, pred, metric))
        time_list.append(time_dict)
    return score_list, time_list</code></pre>
</details>
</dd>
<dt id="asid.automl_imbalanced.tools_ilc.balance_exp"><code class="name flex">
<span>def <span class="ident">balance_exp</span></span>(<span>X, y, skf, bal_alg, alg, hyperopt_time, metric)</span>
</code></dt>
<dd>
<div class="desc"><p>Evaluates model performance on a range of splits.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>array-like</code> of <code>shape (n_samples, n_features)</code></dt>
<dd>Training sample.</dd>
<dt><strong><code>y</code></strong> :&ensp;<code>array-like</code></dt>
<dd>The target values.</dd>
<dt><strong><code>skf</code></strong> :&ensp;<code>instance</code></dt>
<dd>Splitting strategy instance.</dd>
<dt><strong><code>bal_alg</code></strong> :&ensp;<code>str</code></dt>
<dd>Sampling procedure label.</dd>
<dt><strong><code>alg</code></strong> :&ensp;<code>str</code></dt>
<dd>Ensemble classifier label.</dd>
<dt><strong><code>hyperopt_time</code></strong> :&ensp;<code>int</code></dt>
<dd>The runtime setting (in seconds) for Hyperopt optimization.</dd>
<dt><strong><code>metric</code></strong> :&ensp;<code>str</code></dt>
<dd>Metric that is used to evaluate the model performance.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>score_list</code></strong> :&ensp;<code>list</code></dt>
<dd>Model performance on a range of splits.</dd>
<dt><strong><code>time_list</code></strong> :&ensp;<code>list</code></dt>
<dd>Model fitting and prediction time on a range of splits.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def balance_exp(X, y, skf, bal_alg, alg, hyperopt_time, metric):
    &#34;&#34;&#34;
    Evaluates model performance on a range of splits.

    Parameters
    ----------
    X : array-like of shape (n_samples, n_features)
        Training sample.

    y : array-like
        The target values.

    skf : instance
        Splitting strategy instance.

    bal_alg : str
        Sampling procedure label.

    alg : str
        Ensemble classifier label.

    hyperopt_time : int
        The runtime setting (in seconds) for Hyperopt optimization.

    metric : str
        Metric that is used to evaluate the model performance.

    Returns
    -------
    score_list : list
        Model performance on a range of splits.

    time_list : list
         Model fitting and prediction time on a range of splits.
    &#34;&#34;&#34;
    score_list = []
    time_list = []
    for train_index, test_index in skf.split(X, y):
        time_dict = {}
        X_train, X_test = X[train_index], X[test_index]
        y_train, y_test = y[train_index], y[test_index]
        if hyperopt_time != 0:
            balance_params = get_balance_params(X_train, y_train, bal_alg, alg, hyperopt_time, metric)
            balance_params[&#34;balancing__sampling_strategy&#34;] = get_sampl_strat_for_case(
                balance_params[&#34;balancing__sampling_strategy&#34;], np.unique(y_train, return_counts=True), bal_alg)
        else:
            balance_params = None
        X_train_scaled, scaler = scale_data(X_train)
        estimator = Pipeline([
            (&#39;balancing&#39;, balance_dict[bal_alg]),
            (&#39;classification&#39;, classificator_dict[alg])])
        if balance_params:
            estimator.set_params(**balance_params)
        t0 = datetime.now()
        estimator.fit(X_train_scaled, y_train)
        time_dict[&#34;train_time&#34;] = datetime.now() - t0
        X_test_scaled = scaler.transform(X_test)
        if metric in [&#34;roc_auc&#34;, &#34;log_loss&#34;]:
            t0 = datetime.now()
            pred = estimator.predict_proba(X_test_scaled)
            time_dict[&#34;predict_time&#34;] = datetime.now() - t0
            score_list.append(calc_metric(y_test, pred, metric))
        else:
            t0 = datetime.now()
            pred = estimator.predict(X_test_scaled)
            time_dict[&#34;predict_time&#34;] = datetime.now() - t0
            score_list.append(calc_metric(y_test, pred, metric))
        time_list.append(time_dict)
    return score_list, time_list</code></pre>
</details>
</dd>
<dt id="asid.automl_imbalanced.tools_ilc.calc_leaderboard"><code class="name flex">
<span>def <span class="ident">calc_leaderboard</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculates the leaderboard statistics.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>ls</code></strong> :&ensp;<code>dict</code></dt>
<dd>The leaderboard statistics that includes sorted lists in accordance with the following indicators:
"Mean score", "Mean rank", "Share of experiments with the first place, %",
"Average difference with the leader, %".</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def calc_leaderboard(self):
    &#34;&#34;&#34;
    Calculates the leaderboard statistics.

    Returns
    -------
    ls : dict
        The leaderboard statistics that includes sorted lists in accordance with the following indicators:
        &#34;Mean score&#34;, &#34;Mean rank&#34;, &#34;Share of experiments with the first place, %&#34;,
        &#34;Average difference with the leader, %&#34;.
    &#34;&#34;&#34;
    ls = {}
    mean_dict = {}
    sub_rank_list = [[] for el in list(range(self.split_num))]
    for el in list(self.evaluated_models_scores_.keys()):
        mean_dict[el] = np.mean(self.evaluated_models_scores_[el])
        for i, val in enumerate(self.evaluated_models_scores_[el]):
            if self.eval_metric == &#34;log_loss&#34;:
                sub_rank_list[i].append(val)
            else:
                sub_rank_list[i].append(-val)
    if self.eval_metric == &#34;log_loss&#34;:
        mean_dict = {k: v for k, v in sorted(mean_dict.items(), key=lambda item: (item[1]))}
    else:
        mean_dict = {k: v for k, v in sorted(mean_dict.items(), key=lambda item: (item[1]), reverse=True)}
    leader = list(mean_dict.keys())[0]
    ls[&#34;Mean score&#34;] = mean_dict
    model_rank = []
    for i in range(len(sub_rank_list)):
        model_rank.append(rankdata(sub_rank_list[i], method=&#39;dense&#39;))
    model_rank = np.array(model_rank)
    rank_dict = {}
    leader_share = {}
    diff_dict = {}
    for i, el in enumerate(list(self.evaluated_models_scores_.keys())):
        rank_dict[el] = np.mean(model_rank[:, i])
        leader_share[el] = model_rank[model_rank[:, i] == 1, i].shape[0] / model_rank.shape[0] * 100
        if el != leader:
            diff = (np.array(self.evaluated_models_scores_[el]) - np.array(
                self.evaluated_models_scores_[leader])) / np.array(self.evaluated_models_scores_[leader]) * 100
            diff_dict[el] = np.mean(diff)
    rank_dict = {k: v for k, v in sorted(rank_dict.items(), key=lambda item: (item[1]))}
    leader_share = {k: v for k, v in sorted(leader_share.items(), key=lambda item: (item[1]), reverse=True)}
    diff_dict = {k: v for k, v in sorted(diff_dict.items(), key=lambda item: (item[1]), reverse=True)}
    ls[&#34;Mean rank&#34;] = rank_dict
    ls[&#34;Share of experiments with the first place, %&#34;] = leader_share
    ls[&#34;Average difference with the leader, %&#34;] = diff_dict
    return ls</code></pre>
</details>
</dd>
<dt id="asid.automl_imbalanced.tools_ilc.calc_metric"><code class="name flex">
<span>def <span class="ident">calc_metric</span></span>(<span>y_test, pred, metric)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculates the evaluation metric.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>y_test</code></strong> :&ensp;<code>array-like</code></dt>
<dd>Correct target values.</dd>
<dt><strong><code>pred</code></strong> :&ensp;<code>array-like</code></dt>
<dd>Predicted target values.</dd>
<dt><strong><code>metric</code></strong> :&ensp;<code>str</code></dt>
<dd>Metric that is used to evaluate the model performance.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>score</code></strong> :&ensp;<code>float</code></dt>
<dd>Metric value.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def calc_metric(y_test, pred, metric):
    &#34;&#34;&#34;
    Calculates the evaluation metric.

    Parameters
    ----------
    y_test : array-like
        Correct target values.

    pred : array-like
        Predicted target values.

    metric : str
        Metric that is used to evaluate the model performance.

    Returns
    -------
    score : float
        Metric value.
    &#34;&#34;&#34;
    if metric.split(&#34;_&#34;)[0] == &#34;f1&#34;:
        if metric == &#34;f1_macro&#34;:
            score = f1_score(y_test, pred, average=&#34;macro&#34;)
        elif metric == &#34;f1_micro&#34;:
            score = f1_score(y_test, pred, average=&#34;micro&#34;)
        elif metric == &#34;f1_weighted&#34;:
            score = f1_score(y_test, pred, average=&#34;weighted&#34;)
    elif metric == &#34;accuracy&#34;:
        score = accuracy_score(y_test, pred)
    elif metric == &#34;roc_auc&#34;:
        if len(np.unique(y_test)) == 2:
            score = roc_auc_score(y_test, pred[:, 1])
        else:
            score = roc_auc_score(y_test, pred)
    elif metric == &#34;log_loss&#34;:
        score = log_loss(y_test, pred)
    return score</code></pre>
</details>
</dd>
<dt id="asid.automl_imbalanced.tools_ilc.calc_pipeline_acc"><code class="name flex">
<span>def <span class="ident">calc_pipeline_acc</span></span>(<span>params, X, y, bal_alg, alg, metric)</span>
</code></dt>
<dd>
<div class="desc"><p>Evaluates the pipeline.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>params</code></strong> :&ensp;<code>dict</code></dt>
<dd>Parameters generated by Hyperopt.</dd>
<dt><strong><code>X</code></strong> :&ensp;<code>array-like</code> of <code>shape (n_samples, n_features)</code></dt>
<dd>Training sample.</dd>
<dt><strong><code>y</code></strong> :&ensp;<code>array-like</code></dt>
<dd>The target values.</dd>
<dt><strong><code>bal_alg</code></strong> :&ensp;<code>str</code></dt>
<dd>Sampling procedure label.</dd>
<dt><strong><code>alg</code></strong> :&ensp;<code>str</code></dt>
<dd>Ensemble classifier label.</dd>
<dt><strong><code>metric</code></strong> :&ensp;<code>str</code></dt>
<dd>Metric that is used to evaluate the model performance.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>score</code></strong> :&ensp;<code>float</code></dt>
<dd>Evaluation of the model performance.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def calc_pipeline_acc(params, X, y, bal_alg, alg, metric):
    &#34;&#34;&#34;
    Evaluates the pipeline.

    Parameters
    ----------
    params : dict
        Parameters generated by Hyperopt.

    X : array-like of shape (n_samples, n_features)
        Training sample.

    y : array-like
        The target values.

    bal_alg : str
        Sampling procedure label.

    alg : str
        Ensemble classifier label.

    metric : str
        Metric that is used to evaluate the model performance.

    Returns
    -------
    score : float
        Evaluation of the model performance.
    &#34;&#34;&#34;
    score_list = []
    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    count = 0
    for train_index, test_index in skf.split(X, y):
        X_train, X_test = X[train_index], X[test_index]
        y_train, y_test = y[train_index], y[test_index]
        estimator = Pipeline([
            (&#39;balancing&#39;, balance_dict[bal_alg]),
            (&#39;classification&#39;, classificator_dict[alg])])
        if count == 0:
            count_class = np.unique(y_train, return_counts=True)
            params[&#34;balancing__sampling_strategy&#34;] = get_sampl_strat_for_case(params[&#34;balancing__sampling_strategy&#34;],
                                                                              count_class, bal_alg)
            if &#34;balancing__k_neighbors&#34; in params:
                params[&#34;balancing__k_neighbors&#34;] = int(params[&#34;balancing__k_neighbors&#34;])
            elif &#34;balancing__n_neighbors&#34; in params:
                params[&#34;balancing__n_neighbors&#34;] = int(params[&#34;balancing__n_neighbors&#34;])
        estimator.set_params(**params)
        X_train_scaled, scaler = scale_data(X_train)
        try:
            estimator.fit(X_train_scaled, y_train)
            X_test_scaled = scaler.transform(X_test)
            if metric in [&#34;roc_auc&#34;, &#34;log_loss&#34;]:
                pred = estimator.predict_proba(X_test_scaled)
                score_list.append(calc_metric(y_test, pred, metric))
            else:
                pred = estimator.predict(X_test_scaled)
                score_list.append(calc_metric(y_test, pred, metric))
        except:
            if metric == &#34;log_loss&#34;:
                score_list.append(np.inf)
            else:
                score_list.append(0)
        count += 1
    score = np.mean(score_list)
    if metric != &#34;log_loss&#34;:
        score = -score
    return score</code></pre>
</details>
</dd>
<dt id="asid.automl_imbalanced.tools_ilc.choose_and_fit_ilc"><code class="name flex">
<span>def <span class="ident">choose_and_fit_ilc</span></span>(<span>self, X, y)</span>
</code></dt>
<dd>
<div class="desc"><p>Chooses the optimal classifier and fits the resulting estimator.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>array-like</code> of <code>shape (n_samples, n_features)</code></dt>
<dd>Training sample.</dd>
<dt><strong><code>y</code></strong> :&ensp;<code>array-like</code></dt>
<dd>The target values.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>classifer</code></strong> :&ensp;<code>instance</code></dt>
<dd>Optimal fitted classifier.</dd>
<dt><strong><code>option_label</code></strong> :&ensp;<code>str</code></dt>
<dd>Optimal classifier label.</dd>
<dt><strong><code>score</code></strong> :&ensp;<code>float</code></dt>
<dd>Averaged out-of-fold value of eval_metric for the optimal classifier.</dd>
<dt><strong><code>scaler</code></strong> :&ensp;<code>instance</code></dt>
<dd>Fitted scaler that is applied prior to classifier estimation.</dd>
<dt><strong><code>score_dict</code></strong> :&ensp;<code>dict</code></dt>
<dd>Score series for the range of estimated classifiers.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def choose_and_fit_ilc(self, X, y):
    &#34;&#34;&#34;
    Chooses the optimal classifier and fits the resulting estimator.

    Parameters
    ----------
    X : array-like of shape (n_samples, n_features)
        Training sample.

    y : array-like
        The target values.

    Returns
    -------
    classifer : instance
        Optimal fitted classifier.

    option_label : str
        Optimal classifier label.

    score : float
        Averaged out-of-fold value of eval_metric for the optimal classifier.

    scaler : instance
        Fitted scaler that is applied prior to classifier estimation.

    score_dict : dict
        Score series for the range of estimated classifiers.
    &#34;&#34;&#34;
    res_dict = {}
    score_dict = {}
    time_dict = {}
    cv_type = get_cv_type(self.split_num)
    option_list = []
    for alg in [&#34;catboost&#34;, &#34;RF&#34;, &#34;LGBM&#34;, &#34;XGB&#34;]:
        for bal_alg in [&#34;RandomOverSampler&#34;, &#34;SMOTE&#34;, &#34;RandomUnderSampler&#34;, &#34;ADASYN&#34;]:
            score_list, time_list = fit_alg(cv_type, X, y, bal_alg, alg, self.hyperopt_time, self.split_num, self.eval_metric)
            option_list.append(bal_alg + &#34;+&#34; + alg)
            score_dict[option_list[-1]] = score_list
            res_dict[option_list[-1]] = np.mean(score_list)
    score_list, time_list = fit_alg(cv_type, X, y, None, &#34;AutoBalanceBoost&#34;, self.hyperopt_time, self.split_num, self.eval_metric)
    option_list.append(&#34;AutoBalanceBoost&#34;)
    score_dict[option_list[-1]] = score_list
    time_dict[option_list[-1]] = time_list
    res_dict[option_list[-1]] = np.mean(score_list)
    if self.eval_metric == &#34;log_loss&#34;:
        res_dict = {k: v for k, v in sorted(res_dict.items(), key=lambda item: (item[1]))}
    else:
        res_dict = {k: v for k, v in sorted(res_dict.items(), key=lambda item: (item[1]), reverse=True)}
    option_label = list(res_dict.keys())[0]
    score = list(res_dict.values())[0]
    classifier, scaler = fit_res_model(option_label, X, y, self.hyperopt_time, self.eval_metric)
    return classifier, option_label, score, scaler, score_dict, time_dict</code></pre>
</details>
</dd>
<dt id="asid.automl_imbalanced.tools_ilc.fit_alg"><code class="name flex">
<span>def <span class="ident">fit_alg</span></span>(<span>cv_type, X, y, bal_alg, alg, hyperopt_time, split_num, metric)</span>
</code></dt>
<dd>
<div class="desc"><p>Evaluates model performance on a full range of splits.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>cv_type</code></strong> :&ensp;<code>str</code></dt>
<dd>The chosen type of splitting iterations.</dd>
<dt><strong><code>X</code></strong> :&ensp;<code>array-like</code> of <code>shape (n_samples, n_features)</code></dt>
<dd>Training sample.</dd>
<dt><strong><code>y</code></strong> :&ensp;<code>array-like</code></dt>
<dd>The target values.</dd>
<dt><strong><code>bal_alg</code></strong> :&ensp;<code>str</code> or <code>None</code></dt>
<dd>Sampling procedure label.</dd>
<dt><strong><code>alg</code></strong> :&ensp;<code>str</code></dt>
<dd>Ensemble classifier label.</dd>
<dt><strong><code>hyperopt_time</code></strong> :&ensp;<code>int</code></dt>
<dd>The runtime setting (in seconds) for Hyperopt optimization.</dd>
<dt><strong><code>split_num</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of splitting iterations.</dd>
<dt><strong><code>metric</code></strong> :&ensp;<code>str</code></dt>
<dd>Metric that is used to evaluate the model performance.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>score_list</code></strong> :&ensp;<code>list</code></dt>
<dd>Model performance on a range of splits.</dd>
<dt><strong><code>time_list</code></strong> :&ensp;<code>list</code></dt>
<dd>Model fitting and prediction time on a range of splits.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fit_alg(cv_type, X, y, bal_alg, alg, hyperopt_time, split_num, metric):
    &#34;&#34;&#34;
    Evaluates model performance on a full range of splits.

    Parameters
    ----------
    cv_type : str
        The chosen type of splitting iterations.

    X : array-like of shape (n_samples, n_features)
        Training sample.

    y : array-like
        The target values.

    bal_alg : str or None
        Sampling procedure label.

    alg : str
        Ensemble classifier label.

    hyperopt_time : int
        The runtime setting (in seconds) for Hyperopt optimization.

    split_num : int
        The number of splitting iterations.

    metric : str
        Metric that is used to evaluate the model performance.

    Returns
    -------
    score_list : list
        Model performance on a range of splits.

    time_list : list
        Model fitting and prediction time on a range of splits.
    &#34;&#34;&#34;
    score_list = []
    time_list = []
    if cv_type == &#34;split&#34;:
        skf = StratifiedShuffleSplit(n_splits=split_num, test_size=0.2, random_state=42)
        if alg == &#34;AutoBalanceBoost&#34;:
            sub_score_list, sub_time_list = abb_exp(X, y, skf, metric)
        else:
            sub_score_list, sub_time_list = balance_exp(X, y, skf, bal_alg, alg, hyperopt_time, metric)
        score_list.extend(sub_score_list)
        time_list.extend(sub_time_list)
    else:
        random.seed(42)
        seed_val = random.sample(list(range(100000)), split_num // 5)
        for seed in seed_val:
            skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)
            if alg == &#34;AutoBalanceBoost&#34;:
                sub_score_list, sub_time_list = abb_exp(X, y, skf, metric)
            else:
                sub_score_list, sub_time_list = balance_exp(X, y, skf, bal_alg, alg, hyperopt_time, metric)
            score_list.extend(sub_score_list)
            time_list.extend(sub_time_list)
    return score_list, time_list</code></pre>
</details>
</dd>
<dt id="asid.automl_imbalanced.tools_ilc.fit_res_model"><code class="name flex">
<span>def <span class="ident">fit_res_model</span></span>(<span>option_label, X, y, hyp_time, metric)</span>
</code></dt>
<dd>
<div class="desc"><p>Fits the resulting estimator.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>option_label</code></strong> :&ensp;<code>str</code></dt>
<dd>Classifier label.</dd>
<dt><strong><code>X</code></strong> :&ensp;<code>array-like</code> of <code>shape (n_samples, n_features)</code></dt>
<dd>Training sample.</dd>
<dt><strong><code>y</code></strong> :&ensp;<code>array-like</code></dt>
<dd>The target values.</dd>
<dt><strong><code>hyp_time</code></strong> :&ensp;<code>int</code></dt>
<dd>The runtime setting (in seconds) for Hyperopt optimization.</dd>
<dt><strong><code>metric</code></strong> :&ensp;<code>str</code></dt>
<dd>Metric that is used to evaluate the model performance.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>model</code></strong> :&ensp;<code>instance</code></dt>
<dd>Fitted estimator.</dd>
<dt><strong><code>scaler</code></strong> :&ensp;<code>instance</code></dt>
<dd>Fitted scaler.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fit_res_model(option_label, X, y, hyp_time, metric):
    &#34;&#34;&#34;
    Fits the resulting estimator.

    Parameters
    ----------
    option_label : str
        Classifier label.

    X : array-like of shape (n_samples, n_features)
        Training sample.

    y : array-like
        The target values.

    hyp_time : int
        The runtime setting (in seconds) for Hyperopt optimization.

    metric : str
        Metric that is used to evaluate the model performance.

    Returns
    -------
    model : instance
        Fitted estimator.

    scaler : instance
        Fitted scaler.
    &#34;&#34;&#34;
    if option_label == &#34;AutoBalanceBoost&#34;:
        model = AutoBalanceBoost()
        model.fit(X, y)
        scaler = None
    else:
        ol = option_label.split(&#34;+&#34;)
        bal_alg = ol[0]
        alg = ol[1]
        if hyp_time != 0:
            balance_params = get_balance_params(X, y, bal_alg, alg, hyp_time, metric)
        else:
            balance_params = None
        X_scaled, scaler = scale_data(X)
        model = Pipeline([
            (&#39;balancing&#39;, balance_dict[bal_alg]),
            (&#39;classification&#39;, classificator_dict[alg])])
        if balance_params:
            balance_params[&#34;balancing__sampling_strategy&#34;] = get_sampl_strat_for_case(
                balance_params[&#34;balancing__sampling_strategy&#34;], np.unique(y, return_counts=True), bal_alg)
            model.set_params(**balance_params)
        model.fit(X_scaled, y)
    return model, scaler</code></pre>
</details>
</dd>
<dt id="asid.automl_imbalanced.tools_ilc.get_balance_params"><code class="name flex">
<span>def <span class="ident">get_balance_params</span></span>(<span>X, y, bal_alg, alg, hyp_time, metric)</span>
</code></dt>
<dd>
<div class="desc"><p>Searches for optimal hyper-parameters for balancing procedure using Hyperopt.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>array-like</code> of <code>shape (n_samples, n_features)</code></dt>
<dd>Training sample.</dd>
<dt><strong><code>y</code></strong> :&ensp;<code>array-like</code></dt>
<dd>The target values.</dd>
<dt><strong><code>bal_alg</code></strong> :&ensp;<code>str</code></dt>
<dd>Sampling procedure label.</dd>
<dt><strong><code>alg</code></strong> :&ensp;<code>str</code></dt>
<dd>Ensemble classifier label.</dd>
<dt><strong><code>hyp_time</code></strong> :&ensp;<code>int</code></dt>
<dd>The runtime setting (in seconds) for Hyperopt optimization.</dd>
<dt><strong><code>metric</code></strong> :&ensp;<code>str</code></dt>
<dd>Metric that is used to evaluate the model performance.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>best</code></strong> :&ensp;<code>dict</code></dt>
<dd>Optimal hyper-parameters for balancing procedure chosen by Hyperopt.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_balance_params(X, y, bal_alg, alg, hyp_time, metric):
    &#34;&#34;&#34;
    Searches for optimal hyper-parameters for balancing procedure using Hyperopt.

    Parameters
    ----------
    X : array-like of shape (n_samples, n_features)
        Training sample.

    y : array-like
        The target values.

    bal_alg : str
        Sampling procedure label.

    alg : str
        Ensemble classifier label.

    hyp_time : int
        The runtime setting (in seconds) for Hyperopt optimization.

    metric : str
        Metric that is used to evaluate the model performance.

    Returns
    -------
    best : dict
        Optimal hyper-parameters for balancing procedure chosen by Hyperopt.
    &#34;&#34;&#34;
    count_class = np.unique(y, return_counts=True)
    space_ds = space_dict[bal_alg].copy()
    min_cl_arg = np.argmin(count_class[1])
    min_class = count_class[1][min_cl_arg] / np.max(count_class[1])
    space_ds[&#34;balancing__sampling_strategy&#34;] = hp.uniform(&#34;balancing__sampling_strategy&#34;, min_class, 1)
    best = fmin(fn=lambda params: calc_pipeline_acc(params, X, y, bal_alg, alg, metric), space=space_ds,
                algo=tpe.suggest, timeout=hyp_time, rstate=np.random.seed(42))
    best = space_eval(space_ds, best)
    if &#34;balancing__k_neighbors&#34; in best:
        best[&#34;balancing__k_neighbors&#34;] = int(best[&#34;balancing__k_neighbors&#34;])
    elif &#34;balancing__n_neighbors&#34; in best:
        best[&#34;balancing__n_neighbors&#34;] = int(best[&#34;balancing__n_neighbors&#34;])
    return best</code></pre>
</details>
</dd>
<dt id="asid.automl_imbalanced.tools_ilc.get_cv_type"><code class="name flex">
<span>def <span class="ident">get_cv_type</span></span>(<span>split_num)</span>
</code></dt>
<dd>
<div class="desc"><p>Defines the type of splitting iterations.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>split_num</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of splitting iterations.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>cv_type</code></strong> :&ensp;<code>str</code></dt>
<dd>The chosen type of splitting iterations.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_cv_type(split_num):
    &#34;&#34;&#34;
    Defines the type of splitting iterations.

    Parameters
    ----------
    split_num : int
        The number of splitting iterations.

    Returns
    -------
    cv_type : str
        The chosen type of splitting iterations.
    &#34;&#34;&#34;
    if split_num % 5 == 0:
        cv_type = &#34;kfold&#34;
    else:
        cv_type = &#34;split&#34;
    return cv_type</code></pre>
</details>
</dd>
<dt id="asid.automl_imbalanced.tools_ilc.get_sampl_strat_for_case"><code class="name flex">
<span>def <span class="ident">get_sampl_strat_for_case</span></span>(<span>ss, count_class, balance_method)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculates the sampling strategy parameter.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>ss</code></strong> :&ensp;<code>float</code></dt>
<dd>Sampling strategy parameter generated by Hyperopt.</dd>
<dt><strong><code>count_class</code></strong> :&ensp;<code>array-like</code></dt>
<dd>The sorted unique values with the number of counts.</dd>
<dt><strong><code>balance_method</code></strong> :&ensp;<code>str</code></dt>
<dd>Balancing procedure label.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>ss_corr</code></strong> :&ensp;<code>float</code> or <code>dict</code></dt>
<dd>The adjusted sampling strategy parameter.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_sampl_strat_for_case(ss, count_class, balance_method):
    &#34;&#34;&#34;
    Calculates the sampling strategy parameter.

    Parameters
    ----------
    ss : float
        Sampling strategy parameter generated by Hyperopt.

    count_class : array-like
        The sorted unique values with the number of counts.

    balance_method : str
        Balancing procedure label.

    Returns
    -------
    ss_corr : float or dict
        The adjusted sampling strategy parameter.
    &#34;&#34;&#34;
    min_cl_arg = np.argmin(count_class[1])
    max_cl_arg = np.argmax(count_class[1])
    if len(count_class[0]) &gt; 2:
        if balance_method == &#34;RandomUnderSampler&#34;:
            new_dict = {}
            for i, val in enumerate(count_class[0]):
                if i == min_cl_arg:
                    new_dict[val] = count_class[1][i]
                else:
                    new_dict[val] = min(int(round(count_class[1][min_cl_arg] / ss, 0)), count_class[1][i])
        else:
            new_dict = {}
            for i, val in enumerate(count_class[0]):
                if i == max_cl_arg:
                    new_dict[val] = count_class[1][i]
                else:
                    new_dict[val] = max(int(round(count_class[1][max_cl_arg] * ss, 0)), count_class[1][i])
        ss_corr = new_dict
    else:
        ss_corr = max(count_class[1][min_cl_arg] / count_class[1][max_cl_arg], ss)
    return ss_corr</code></pre>
</details>
</dd>
<dt id="asid.automl_imbalanced.tools_ilc.scale_data"><code class="name flex">
<span>def <span class="ident">scale_data</span></span>(<span>X_train)</span>
</code></dt>
<dd>
<div class="desc"><p>Fits scaler and applies it to the train sample.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>X_train</code></strong> :&ensp;<code>array-like</code> of <code>shape (n_samples, n_features)</code></dt>
<dd>Training sample.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>X_train_scaled</code></strong> :&ensp;<code>array-like</code> of <code>shape (n_samples, n_features)</code></dt>
<dd>Scaled sample.</dd>
<dt><strong><code>scaler</code></strong> :&ensp;<code>instance</code></dt>
<dd>Fitted scaler.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def scale_data(X_train):
    &#34;&#34;&#34;
    Fits scaler and applies it to the train sample.

    Parameters
    ----------
    X_train : array-like of shape (n_samples, n_features)
        Training sample.

    Returns
    -------
    X_train_scaled : array-like of shape (n_samples, n_features)
        Scaled sample.

    scaler : instance
        Fitted scaler.
    &#34;&#34;&#34;
    scaler = StandardScaler()
    scaler.fit(X_train)
    X_train_scaled = scaler.transform(X_train)
    return X_train_scaled, scaler</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="asid.automl_imbalanced" href="index.html">asid.automl_imbalanced</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="asid.automl_imbalanced.tools_ilc.abb_exp" href="#asid.automl_imbalanced.tools_ilc.abb_exp">abb_exp</a></code></li>
<li><code><a title="asid.automl_imbalanced.tools_ilc.balance_exp" href="#asid.automl_imbalanced.tools_ilc.balance_exp">balance_exp</a></code></li>
<li><code><a title="asid.automl_imbalanced.tools_ilc.calc_leaderboard" href="#asid.automl_imbalanced.tools_ilc.calc_leaderboard">calc_leaderboard</a></code></li>
<li><code><a title="asid.automl_imbalanced.tools_ilc.calc_metric" href="#asid.automl_imbalanced.tools_ilc.calc_metric">calc_metric</a></code></li>
<li><code><a title="asid.automl_imbalanced.tools_ilc.calc_pipeline_acc" href="#asid.automl_imbalanced.tools_ilc.calc_pipeline_acc">calc_pipeline_acc</a></code></li>
<li><code><a title="asid.automl_imbalanced.tools_ilc.choose_and_fit_ilc" href="#asid.automl_imbalanced.tools_ilc.choose_and_fit_ilc">choose_and_fit_ilc</a></code></li>
<li><code><a title="asid.automl_imbalanced.tools_ilc.fit_alg" href="#asid.automl_imbalanced.tools_ilc.fit_alg">fit_alg</a></code></li>
<li><code><a title="asid.automl_imbalanced.tools_ilc.fit_res_model" href="#asid.automl_imbalanced.tools_ilc.fit_res_model">fit_res_model</a></code></li>
<li><code><a title="asid.automl_imbalanced.tools_ilc.get_balance_params" href="#asid.automl_imbalanced.tools_ilc.get_balance_params">get_balance_params</a></code></li>
<li><code><a title="asid.automl_imbalanced.tools_ilc.get_cv_type" href="#asid.automl_imbalanced.tools_ilc.get_cv_type">get_cv_type</a></code></li>
<li><code><a title="asid.automl_imbalanced.tools_ilc.get_sampl_strat_for_case" href="#asid.automl_imbalanced.tools_ilc.get_sampl_strat_for_case">get_sampl_strat_for_case</a></code></li>
<li><code><a title="asid.automl_imbalanced.tools_ilc.scale_data" href="#asid.automl_imbalanced.tools_ilc.scale_data">scale_data</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>